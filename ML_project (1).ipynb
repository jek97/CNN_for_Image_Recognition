{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Project #\n",
        "## 1. Introduction ##\n",
        "in this notebok we will create a 8 layer convolutional neural netweork for the image classification of animals.\n",
        "we build this project using the Keras API, the data is first uploaded ([Dataset](https://www.kaggle.com/datasets/pratik2901/animal-dataset)) and preprocessed, secondly the neural network has been created and feeded with the data, finally the results are analysed.\n"
      ],
      "metadata": {
        "id": "CGuzYXhw1tdd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw-oQ7kSGDuR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(2)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.optimizers import RMSprop, SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "sns.set(style='white', context='notebook', palette='deep')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset upload and Preprocessing ##\n"
      ],
      "metadata": {
        "id": "j5VYwA-H475T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount the driver\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O53qq4ZCLTuj",
        "outputId": "55ea1597-1ea3-4160-df5a-8d43709b0f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the files path\n",
        "train_dir='/content/drive/MyDrive/machine learning/dataset/animal_dataset_intermediate/train/'\n",
        "test_dir='/content/drive/MyDrive/machine learning/dataset/animal_dataset_intermediate/test'"
      ],
      "metadata": {
        "id": "w7eqowpdK7V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Label Preprocessing ###\n",
        "the dataset is organized in 5 folders, each one containing the images of a given animal type, the name of such animal is encoded in the direcories name, we can proceed extracting all these labels and associating them to a number.\n",
        "then we will convert each label number to a hot vector."
      ],
      "metadata": {
        "id": "VTTZA76jTbo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the labels from the directories names\n",
        "Name=[]\n",
        "for file in os.listdir(train_dir):\n",
        "    Name+=[file]\n",
        "print(Name)\n",
        "print(len(Name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSohUeMyK8Y4",
        "outputId": "34525ca4-7625-4150-fadf-14025db7d32e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['elefante_train', 'farfalla_train', 'mucca_train', 'pecora_train', 'scoiattolo_train']\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# associate a number to each label\n",
        "N=[]\n",
        "for i in range(len(Name)):\n",
        "    N+=[i]\n",
        "normal_mapping=dict(zip(Name,N))\n",
        "print(normal_mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKeA_T6pL96q",
        "outputId": "0fa68cfe-4390-4c99-fd3b-1498d0c62e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'elefante_train': 0, 'farfalla_train': 1, 'mucca_train': 2, 'pecora_train': 3, 'scoiattolo_train': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Images Preprocessing ###\n",
        "the colored images are encoded as an array of uniform dimensions of 48 x 48 with normalized valeus of the color components, obtaining a list object of dimension 8196 x 48 x 48 x 3."
      ],
      "metadata": {
        "id": "nXArSx_I6lzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing of the dataset\n",
        "trainx0=[]\n",
        "trainy0=[]\n",
        "count=0\n",
        "for file in Name:\n",
        "    path=os.path.join(train_dir,file)\n",
        "    for im in tqdm(os.listdir(path)):\n",
        "        if im[-4:]!='.txt':\n",
        "            image=load_img(os.path.join(path,im), grayscale=False, color_mode='rgb', target_size=(48,48))\n",
        "            image=img_to_array(image)\n",
        "            image=image/255.0 # normalization\n",
        "            trainx0.append(image)\n",
        "            trainy0.append(count)\n",
        "    count=count+1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k--evDnaMZZF",
        "outputId": "21525871-03f9-4992-efe0-e31ec2c91ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1302/1302 [00:50<00:00, 25.54it/s] \n",
            "100%|██████████| 1901/1901 [00:51<00:00, 36.82it/s] \n",
            "100%|██████████| 1680/1680 [00:29<00:00, 57.39it/s] \n",
            "100%|██████████| 1639/1639 [00:28<00:00, 57.55it/s] \n",
            "100%|██████████| 1676/1676 [00:28<00:00, 59.67it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0])\n",
        "Y_train = to_categorical(trainy0, num_classes = 5)"
      ],
      "metadata": {
        "id": "tJuonVRMQoUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Dataset Splitting ###\n",
        "we can devide our dataset in training (70%), validation (20%) and test (10%) sets."
      ],
      "metadata": {
        "id": "S7jyxcmmRaR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "random_seed_t = 2\n",
        "random_seed_v = 4\n",
        "# Split the train, the validation and the test set for the fitting\n",
        "X_train, X_ve, Y_train, Y_ve = train_test_split(trainx0, Y_train, test_size = 0.3, random_state=random_seed_t)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_ve, Y_ve, test_size = 0.33, random_state=random_seed_v)"
      ],
      "metadata": {
        "id": "dW9qx69hQabc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = plt.imshow(X_train[10][:,:,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "0bBVxRpORg9T",
        "outputId": "7fba10ae-da8d-40d2-d64d-d879a37c9bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. CNN Design Training and Testing ##\n",
        "### 3.1.1 Neural Network Architecture ###\n",
        "we design the Neural Network taking advantages of the Keras **Sequential** API which is really intuitive and allows to create a sequential NN by adding one layer per time, we started with two **Convolutional** layers that isolate the image features; \\\n",
        "after them a **MaxPool** layer is used to reduce the data sizes by one half, then the first implicit layer, the **Dropout** one, is placed to implicity regularize the model, by randomly set the 30% of the input and output of the neurons to zero. \\\n",
        "the same layers organization is riproposed a second time with different paramethers. \\\n",
        "going forward some layers for the reshape of the data are placed, the **Flatten** layer perform this operation leading to a common shape of all the tensors, while the **Dense** layer perform a matrix multiplication with the same purpose. \\\n",
        "The last Dense layer cover an important role in our architecture since it will shape the data to the size of the hot vector related to the labels and thanks to the softmax activation it will conver the data to a probability distribution of the 5 possible classes.   "
      ],
      "metadata": {
        "id": "7s-MkyWNT1FU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the CNN model\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',\n",
        "                 activation ='relu', input_shape = (48,48,3)))\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',\n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',\n",
        "                 activation ='relu'))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',\n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.35))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(5, activation = \"softmax\"))\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "XP0eNvABSmI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ae7868-3523-44cc-9060-c902bda3d118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 48, 48, 32)        2432      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 48, 48, 32)        25632     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 24, 24, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 24, 24, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 24, 24, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 12, 12, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 9216)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               2359552   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,444,325\n",
            "Trainable params: 2,444,325\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2 Optimizer ###\n",
        "the output of the NN is then used by the optimizer for the training of itself, in detail the **RMSprop** optimizer takes in input the hot vector obtained from the NN and the related true one present in the dataset and evaluate the distance between the two. \\\n",
        "this distance is then used in backpropagation for the tuning of the weights.\\\n",
        "[More informations](https://deepchecks.com/glossary/rmsprop/#:~:text=RMSProp%20algorithm,gradient%20to%20minimize%20the%20loss.)"
      ],
      "metadata": {
        "id": "LTx0W2mMYWVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer\n",
        "optimizer = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08, decay=0.0)"
      ],
      "metadata": {
        "id": "DZ_NoQ2eTTV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "vJpHqLYjTcQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.3 Learning Rate Reduction ###\n",
        "commonly used in artificial neural network, the learning rate can be seen as the amount of which the weights are modified respect the error obtained from the optimizer.\\\n",
        "this amount can be changed in case of plateau of the accuracy, in our case, to influence less the weights. \\\n",
        "[More informations](https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/)"
      ],
      "metadata": {
        "id": "dVKqz6skZHKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a learning rate annealer\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', # val acc\n",
        "                                            patience=4,\n",
        "                                            verbose=1,\n",
        "                                            factor=0.35,\n",
        "                                            min_lr=0.00001)"
      ],
      "metadata": {
        "id": "6T8mX5B9Tfyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.4 Setting the Epochs and the Batch size ###\n",
        "the model will be trained on many subsection of the dataset composed by a random composition of images (samples) equal to the **batch size**.\\\n",
        "on the other side the **epochs** shows the number of times the whole dataset as been checked by the system during the training.\\\n",
        "You can think of a for-loop over the number of epochs where each loop proceeds over the training dataset. Within this for-loop is another nested for-loop that iterates over each batch of samples, where one batch has the specified “batch size” number of samples. \\\n",
        "[more informations](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/#:~:text=at%20an%20epoch.-,What%20Is%20an%20Epoch%3F,update%20the%20internal%20model%20parameters.)"
      ],
      "metadata": {
        "id": "7Zh1W4Rnanbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tuning the training\n",
        "epochs = 30\n",
        "batch_size = 90"
      ],
      "metadata": {
        "id": "6JUk9LXrThWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.5 Data Augmentation ###\n",
        "since the dataset dimension is not too big we decided to perform a data augmentation, this operation will take the images, warp and rotate them enlarging their number, this operation will not only increate the dimension of the dataset but also make the final model more robust."
      ],
      "metadata": {
        "id": "VMCuo4EjZspC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With data augmentation to prevent overfitting\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180) (it was 10)\n",
        "        zoom_range = 0.1, # Randomly zoom image\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=False,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "datagen.fit(X_train)\n"
      ],
      "metadata": {
        "id": "g9f9CUinTqDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "history = model.fit_generator(datagen.flow(np.array(X_train),Y_train, batch_size=batch_size),\n",
        "                              epochs = epochs, validation_data = (np.array(X_val),Y_val),\n",
        "                              verbose = 2, steps_per_epoch=np.array(X_train).shape[0] // batch_size\n",
        "                              , callbacks=[learning_rate_reduction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7EWVGXoWTzCY",
        "outputId": "a0f3998a-fcc4-459f-9572-5dc88842c6ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-0e5935407616>:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(datagen.flow(np.array(X_train),Y_train, batch_size=batch_size),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "63/63 - 161s - loss: 1.6060 - accuracy: 0.2842 - val_loss: 1.5296 - val_accuracy: 0.3339 - lr: 0.0010 - 161s/epoch - 3s/step\n",
            "Epoch 2/30\n",
            "63/63 - 141s - loss: 1.4368 - accuracy: 0.3875 - val_loss: 1.3606 - val_accuracy: 0.4287 - lr: 0.0010 - 141s/epoch - 2s/step\n",
            "Epoch 3/30\n",
            "63/63 - 140s - loss: 1.3320 - accuracy: 0.4438 - val_loss: 1.2307 - val_accuracy: 0.4669 - lr: 0.0010 - 140s/epoch - 2s/step\n",
            "Epoch 4/30\n",
            "63/63 - 142s - loss: 1.2971 - accuracy: 0.4627 - val_loss: 1.1393 - val_accuracy: 0.5549 - lr: 0.0010 - 142s/epoch - 2s/step\n",
            "Epoch 5/30\n",
            "63/63 - 138s - loss: 1.1815 - accuracy: 0.5127 - val_loss: 1.1239 - val_accuracy: 0.5525 - lr: 0.0010 - 138s/epoch - 2s/step\n",
            "Epoch 6/30\n",
            "63/63 - 141s - loss: 1.1235 - accuracy: 0.5484 - val_loss: 0.9555 - val_accuracy: 0.6308 - lr: 0.0010 - 141s/epoch - 2s/step\n",
            "Epoch 7/30\n",
            "63/63 - 142s - loss: 1.0933 - accuracy: 0.5631 - val_loss: 1.0890 - val_accuracy: 0.5549 - lr: 0.0010 - 142s/epoch - 2s/step\n",
            "Epoch 8/30\n",
            "63/63 - 139s - loss: 1.0308 - accuracy: 0.5947 - val_loss: 0.9270 - val_accuracy: 0.6296 - lr: 0.0010 - 139s/epoch - 2s/step\n",
            "Epoch 9/30\n",
            "63/63 - 141s - loss: 1.0222 - accuracy: 0.6003 - val_loss: 1.0038 - val_accuracy: 0.6060 - lr: 0.0010 - 141s/epoch - 2s/step\n",
            "Epoch 10/30\n",
            "63/63 - 142s - loss: 0.9930 - accuracy: 0.6141 - val_loss: 0.9089 - val_accuracy: 0.6527 - lr: 0.0010 - 142s/epoch - 2s/step\n",
            "Epoch 11/30\n",
            "63/63 - 140s - loss: 0.9709 - accuracy: 0.6161 - val_loss: 1.0806 - val_accuracy: 0.5962 - lr: 0.0010 - 140s/epoch - 2s/step\n",
            "Epoch 12/30\n",
            "63/63 - 138s - loss: 0.9396 - accuracy: 0.6331 - val_loss: 0.8465 - val_accuracy: 0.6697 - lr: 0.0010 - 138s/epoch - 2s/step\n",
            "Epoch 13/30\n",
            "63/63 - 140s - loss: 0.9187 - accuracy: 0.6457 - val_loss: 0.9557 - val_accuracy: 0.6260 - lr: 0.0010 - 140s/epoch - 2s/step\n",
            "Epoch 14/30\n",
            "63/63 - 141s - loss: 0.9132 - accuracy: 0.6510 - val_loss: 0.8711 - val_accuracy: 0.6642 - lr: 0.0010 - 141s/epoch - 2s/step\n",
            "Epoch 15/30\n",
            "63/63 - 138s - loss: 0.8835 - accuracy: 0.6540 - val_loss: 0.8501 - val_accuracy: 0.6715 - lr: 0.0010 - 138s/epoch - 2s/step\n",
            "Epoch 16/30\n",
            "63/63 - 141s - loss: 0.8651 - accuracy: 0.6658 - val_loss: 0.8886 - val_accuracy: 0.6472 - lr: 0.0010 - 141s/epoch - 2s/step\n",
            "Epoch 17/30\n",
            "63/63 - 137s - loss: 0.8412 - accuracy: 0.6726 - val_loss: 0.8460 - val_accuracy: 0.6831 - lr: 0.0010 - 137s/epoch - 2s/step\n",
            "Epoch 18/30\n",
            "63/63 - 139s - loss: 0.8262 - accuracy: 0.6874 - val_loss: 0.8997 - val_accuracy: 0.6551 - lr: 0.0010 - 139s/epoch - 2s/step\n",
            "Epoch 19/30\n",
            "63/63 - 141s - loss: 0.8168 - accuracy: 0.6892 - val_loss: 0.9381 - val_accuracy: 0.6539 - lr: 0.0010 - 141s/epoch - 2s/step\n",
            "Epoch 20/30\n",
            "63/63 - 139s - loss: 0.7957 - accuracy: 0.6982 - val_loss: 0.7917 - val_accuracy: 0.6940 - lr: 0.0010 - 139s/epoch - 2s/step\n",
            "Epoch 21/30\n",
            "63/63 - 139s - loss: 0.7890 - accuracy: 0.6998 - val_loss: 0.7768 - val_accuracy: 0.7116 - lr: 0.0010 - 139s/epoch - 2s/step\n",
            "Epoch 22/30\n",
            "63/63 - 140s - loss: 0.7830 - accuracy: 0.6990 - val_loss: 0.7987 - val_accuracy: 0.6867 - lr: 0.0010 - 140s/epoch - 2s/step\n",
            "Epoch 23/30\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0e5935407616>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit_generator(datagen.flow(np.array(X_train),Y_train, batch_size=batch_size),\n\u001b[0m\u001b[1;32m      3\u001b[0m                               \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                               , callbacks=[learning_rate_reduction])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2634\u001b[0m             \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m         )\n\u001b[0;32m-> 2636\u001b[0;31m         return self.fit(\n\u001b[0m\u001b[1;32m   2637\u001b[0m             \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 Training Considerations ###\n",
        "The following two graphs will show the performances of the model on the training and validation set at the vary of the epochs during the training.\\\n",
        "this curve cover an important role for the design of the architecture, indeed observing the **Loss curve** as we may expect the training loss decrease, since we are searching for a minima of such function, while the red curve forllow the first, this is a good behavior of our model, if oterwise the two curves would diverge we would recognize: \\\n",
        "1. in case of a validation loss diverging above the training one we would have an **overfitting** situation, where the model is memorizing the training data.\\\n",
        "2. in case of a validation loss diverging below the training one we would have an **underfitting** situation, where the model is taking in consideration only a little part of the information of the dataset. \\\n",
        "Moreover it's important to not exchange the accuracy displayed here for the training and the validation as the real accuracy of the model, intdeed these quantities here are biassed, the final accuracy of the model will be evaluated below."
      ],
      "metadata": {
        "id": "1BCzXZF9dKl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss and accuracy curves for training and validation\n",
        "fig, ax = plt.subplots(2,1)\n",
        "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
        "ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
        "legend = ax[0].legend(loc='best', shadow=True)\n",
        "\n",
        "ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
        "ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
        "legend = ax[1].legend(loc='best', shadow=True)"
      ],
      "metadata": {
        "id": "1trOQnRDU6m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Look at confusion matrix on validation\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "# Predict the values from the validation dataset\n",
        "Y_pred = model.predict(np.array(X_val))\n",
        "# Convert predictions classes to one hot vectors\n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1)\n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(Y_val,axis = 1)\n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
        "# plot the confusion matrix\n",
        "plot_confusion_matrix(confusion_mtx, classes = range(5))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gk-IXTHwU7qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display some error results\n",
        "\n",
        "# Predict the values from the validation dataset\n",
        "Y_pred = model.predict(np.array(X_val))\n",
        "# Convert predictions classes to one hot vectors\n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1)\n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(Y_val,axis = 1)\n",
        "# Errors are difference between predicted labels and true labels\n",
        "errors = (Y_pred_classes - Y_true != 0)\n",
        "\n",
        "Y_pred_classes_errors = Y_pred_classes[errors]\n",
        "Y_pred_errors = Y_pred[errors]\n",
        "Y_true_errors = Y_true[errors]\n",
        "#X_val1 = np.array(X_val)\n",
        "X_val_errors = np.array(X_val)[errors]\n",
        "\n",
        "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
        "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
        "    n = 0\n",
        "    nrows = 2\n",
        "    ncols = 3\n",
        "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
        "    for row in range(nrows):\n",
        "        for col in range(ncols):\n",
        "            error = errors_index[n]\n",
        "            ax[row,col].imshow((img_errors[error]).reshape((48,48,3)))\n",
        "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
        "            n += 1\n",
        "\n",
        "# Probabilities of the wrong predicted numbers\n",
        "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
        "\n",
        "# Predicted probabilities of the true values in the error set\n",
        "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
        "\n",
        "# Difference between the probability of the predicted label and the true label\n",
        "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
        "\n",
        "# Sorted list of the delta prob errors\n",
        "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
        "\n",
        "# Top 6 errors\n",
        "most_important_errors = sorted_dela_errors[-6:]\n",
        "\n",
        "# Show the top 6 errors\n",
        "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
      ],
      "metadata": {
        "id": "gebVsRdeVKsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.1 Model Testing ###\n",
        "once obtained the model we can test it on the test set to evaluate its performances, measuring its accuracy and confuzion matrix.\\\n",
        "indeed while the accuracy of the model is a good paramether to summarize the performance of the model, the confusion matrix gives more clues about the errors, showing which are the animal on which the errors are mainly committed."
      ],
      "metadata": {
        "id": "zoLWatnFizu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict results\n",
        "results = model.predict(np.array(X_test))\n",
        "\n",
        "# select the indix with the maximum probability\n",
        "results = np.argmax(results,axis = 1)\n",
        "\n",
        "# accuracy evaluation\n",
        "Y_test = np.argmax(Y_test, axis=1)\n",
        "\n",
        "error = 0\n",
        "for i in range(len(Y_test)):\n",
        "  if(Y_test[i] != results[i]):\n",
        "    error += 1\n",
        "accuracy = (len(Y_test) - error)/len(Y_test)\n",
        "print(\"number of errors: \" + str(error))\n",
        "print(\"accuracy: \" + str(accuracy))\n",
        "print(\"global error: \" + str((1/accuracy)))"
      ],
      "metadata": {
        "id": "YfpvYv4-VTLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at confusion matrix on testing\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_test, results)\n",
        "# plot the confusion matrix\n",
        "plot_confusion_matrix(confusion_mtx, classes = range(5))"
      ],
      "metadata": {
        "id": "8kL0EnEwIwBf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}